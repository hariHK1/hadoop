# -*- coding: utf-8 -*-
"""upload.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1miJwibtXgVmo6dswDd0zqeQHKFmcjV9J
"""
import os
from os import system
# Takes some time 3-5 minutes
#!apt-get install openjdk-8-jdk-headless -qq > /dev/null
#!wget -q https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
#!tar xf spark-3.4.1-bin-hadoop3.tgz

system("pip install -q findspark")
#system("pip install hdfs pandas matplotlib seaborn numpy")
system("pip install hdfs")
system("pip install pandas")
system("pip install matplotlib")
system("pip install seaborn")
system("pip install numpy")

import findspark
#os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
#os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"
findspark.init()

#!pip install hdfs
import hdfs

from hdfs import InsecureClient

# Create an HDFS client
client = InsecureClient('http://117.53.45.158:9870', user='root')

# Specify the local file path and the HDFS destination path
hdfs_destination_path = '/diabetes.csv'  # Adjust the HDFS path as needed
local_file_path = 'diabetes.csv'

# Upload the file to HDFS
client.download(hdfs_destination_path, local_file_path)

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("diabetes").getOrCreate()

df = spark.read.csv("diabetes.csv", header=True, inferSchema=True)

df.show()

df.printSchema()

df.describe().toPandas()

df.groupby('Outcome').count().show()

import matplotlib.pyplot as plt
import seaborn as sns

fig = plt.figure(figsize=(25, 15))
st = fig.suptitle("Distribution of Features", fontsize=50, verticalalignment="center")
for col, num in zip(df.toPandas().describe().columns, range(1,11)):
  ax = fig.add_subplot(3,4, num)
  ax.hist(df.toPandas()[col])
  plt.grid(False)
  plt.xticks(rotation=45, fontsize=20)
  plt.yticks(fontsize=15)
  plt.title(col.upper(), fontsize=20)

plt.tight_layout()
st.set_y(0.95)
fig.subplots_adjust(top=0.85, hspace=0.4)
plt.show()

from pyspark.sql.functions import isnan, when, count, col

df.select([count(when(isnan(c),c)).alias(c) for c in df.columns]).toPandas().head()

"""#Correlation"""

numeric_features = [t[0] for t in df.dtypes if t[1] !='string']
numeric_features_df = df.select(numeric_features)
numeric_features_df.toPandas().head()

col_names = numeric_features_df.columns
features = numeric_features_df.rdd.map(lambda row: row[0:])

from pyspark.mllib.stat import Statistics
import pandas as pd

corr_mat = Statistics.corr(features, method="pearson")
corr_df = pd.DataFrame(corr_mat)
corr_df.index = col_names
corr_df.columns = col_names
round(corr_df, 2)

from pyspark.sql.functions import corr

#sns.heatmap(corr_df);
sns.heatmap(corr_df, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.savefig("my_plot.png", dpi=300)  # "my_plot.png" is the file name

local_file_path = 'my_plot.png'
hdfs_destination_path = '/my_plot.png'  # Adjust the HDFS path as needed

# Upload the file to HDFS
client.upload(hdfs_destination_path, local_file_path)
